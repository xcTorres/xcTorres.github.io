---
layout:     post
title:      "é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œLSTM"
date:       2021-11-14
author:     "xcTorres"
header-img: "img/in-post/machine-learning.jpeg"
catalog:    true
mathjax: true
tags:
    - NLP
    - Machine Learning
---   
## ç®—æ³•  
ç›¸æ¯”RNNåªæœ‰ä¸€ä¸ªä¼ é€’çŠ¶æ€$h^t$ï¼ŒLSTMæœ‰ä¸¤ä¸ªä¼ è¾“çŠ¶æ€ï¼Œä¸€ä¸ª$C^t$ï¼ˆcell stateï¼‰ï¼Œå’Œä¸€ä¸ª$h^t$ï¼ˆhidden stateï¼‰ã€‚RNNä¸­çš„$h^t$å¯¹äºLSTMä¸­çš„$C^t$ï¼‰

å…¶ä¸­å¯¹äºä¼ é€’ä¸‹å»çš„$C^t$æ”¹å˜å¾—å¾ˆæ…¢ï¼Œé€šå¸¸è¾“å‡ºçš„$C^t$æ˜¯ä¸Šä¸€ä¸ªçŠ¶æ€ä¼ è¿‡æ¥çš„$C^{t-1}$åŠ ä¸Šä¸€äº›æ•°å€¼ï¼Œè¿™ä¹Ÿæ˜¯LSTMç›¸æ¯”RNNä¸ºä»€ä¹ˆèƒ½ç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„åŸå› ã€‚è€Œ$h^t$åˆ™åœ¨ä¸åŒèŠ‚ç‚¹ä¸‹å¾€å¾€ä¼šæœ‰å¾ˆå¤§çš„åŒºåˆ«ã€‚

![demo](/img/in-post/lstm/lstm-chain.png)  

æ›´æ–°cell stateçš„å…¬å¼å¦‚ä¸‹ï¼Œç»†èƒçŠ¶æ€$ğ¶^ğ‘¡$ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯$ğ¶^{ğ‘¡âˆ’1}$å’Œé—å¿˜é—¨è¾“å‡º$ğ‘“^ğ‘¡$çš„ä¹˜ç§¯ï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯è¾“å…¥é—¨çš„$ğ‘–^ğ‘¡$å’Œ$ğ‘^ğ‘¡$çš„ä¹˜ç§¯ã€‚
\$\$
    C^{(t)} = C^{(t-1)} \odot f^{(t)} + i^{(t)} \odot a^{(t)} 
\$\$

--- 
è¾“å…¥é—¨ä¸¤éƒ¨åˆ†ç»„æˆ$ğ‘–^ğ‘¡$å’Œ$ğ‘^ğ‘¡$å…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­$ğ‘–^ğ‘¡$çš„æ¿€æ´»å‡½æ•°ä¸ºsigmoidå‡½æ•°ï¼Œ$ğ‘^ğ‘¡$ä¸ºtanhå‡½æ•°ã€‚é—å¿˜é—¨éƒ¨åˆ†$f^{(t)}$å¦‚ä¸‹ï¼Œå…¶ä¸­ğ‘Š,ğ‘ˆ,ğ‘ä¸ºçº¿æ€§å…³ç³»çš„ç³»æ•°å’Œåå€šã€‚
\$\$
    i^{(t)} = \sigma(W_ih^{(t-1)} + U_ix^{(t)} + b_i) 
\$\$

\$\$
    a^{(t)} =tanh(W_ah^{(t-1)} + U_ax^{(t)} + b_a)
\$\$

\$\$
f^{(t)} = \sigma(W_fh^{(t-1)} + U_fx^{(t)} + b_f)
\$\$

--- 
è¾“å‡ºé—¨å…¬å¼å¦‚ä¸‹ï¼Œå¯ä»¥çœ‹å‡ºéšçŠ¶æ€$h^{(t)}$æ˜¯ç»†èƒçŠ¶æ€$C^{(t)}$çš„å‡½æ•°
\$\$
o^{(t)} = \sigma(W_oh^{(t-1)} + U_ox^{(t)} + b_o)
\$\$

\$\$
h^{(t)} = o^{(t)} \odot tanh(C^{(t)})
\$\$


## ä»£ç   
å…³äºè¯¥ç¤ºä¾‹çš„æºç å¯ä»å¦‚ä¸‹githubé“¾æ¥ä¸‹è½½, [https://github.com/xcTorres/machine_learning/blob/master/lstm.ipynb](https://github.com/xcTorres/machine_learning/blob/master/lstm.ipynb)ã€‚

#### è¯»å–æ•°æ®
```python
import numpy as np
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import train_test_split

df = pd.read_csv('./data/ner_dataset.csv', encoding = "ISO-8859-1")
df = df.fillna(method='ffill')
df.head()

class SentenceGetter(object):
    
    def __init__(self, data):
        self.n_sent = 1
        self.data = data
        self.empty = False
        agg_func = lambda s: (s['Word'].values.tolist(), s['Tag'].values.tolist())
        self.grouped = self.data.groupby('Sentence #').apply(agg_func)
        self.sentences = [s for s in self.grouped]
        
    def get_next(self):
        try: 
            s = self.grouped['Sentence: {}'.format(self.n_sent)]
            self.n_sent += 1
            return s 
        except:
            return None
getter = SentenceGetter(df)
```

#### è¯è½¬ç´¢å¼•
```python
def prepare_sequence(seq, to_ix):
    idxs = [to_ix[w] for w in seq]
    return torch.tensor(idxs, dtype=torch.long)

word_to_ix = {}
tag_to_ix = {}

# For each words-list (sentence) and tags-list in each tuple of training_data
for sent, tags in getter.sentences:
    for word in sent:
        if word not in word_to_ix:  # word has not been assigned an index yet
            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index
    for tag in tags:
        if tag not in tag_to_ix:
            tag_to_ix[tag] = len(tag_to_ix)
            
print(tag_to_ix)
```

#### å®šä¹‰LSTMç»“æ„
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

EMBEDDING_DIM = 32
HIDDEN_DIM = 32  


class LSTMTagger(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):
        super(LSTMTagger, self).__init__()
        self.hidden_dim = hidden_dim

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        # The LSTM takes word embeddings as inputs, and outputs hidden states
        # with dimensionality hidden_dim.
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        # The linear layer that maps from hidden state space to tag space
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence)
        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))
        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_space, dim=1)
        return tag_scores

```

#### è®­ç»ƒ
```python
from sklearn.model_selection import train_test_split
X = [s[0] for s in getter.sentences]
y = [s[1] for s in getter.sentences]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)

model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

for epoch in range(20):
    print(epoch)
    for sentence, tags in zip(X_train, y_train):
        # Step 1. Remember that Pytorch accumulates gradients.
        # We need to clear them out before each instance
        model.zero_grad()

        # Step 2. Get our inputs ready for the network, that is, turn them into
        # Tensors of word indices.
        sentence_in = prepare_sequence(sentence, word_to_ix)
        targets = prepare_sequence(tags, tag_to_ix)

        # Step 3. Run our forward pass.
        tag_scores = model(sentence_in)

        # Step 4. Compute the loss, gradients, and update the parameters by
        #  calling optimizer.step()
        loss = loss_function(tag_scores, targets)
        loss.backward()
        optimizer.step()
```

#### é¢„æµ‹
```python
import numpy as np
label_pred = np.array([])

with torch.no_grad():
    for t in X_test:
        inputs = prepare_sequence(t, word_to_ix)
        tag_score = model(inputs)
        label_pred = np.append(label_pred, torch.argmax(tag_score, dim=1).numpy())

y = []
for i in y_test:
    for w in i:
        y.append(tag_to_ix[w])

from sklearn.metrics import f1_score
f1_score(label_pred, y, average='weighted')
```



## Reference  
[https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
[https://www.cnblogs.com/pinard/p/6519110.html](https://www.cnblogs.com/pinard/p/6519110.html)  
[RNNåå‘ä¼ æ’­æ¨å¯¼](https://www.cnblogs.com/pinard/p/6509630.html)  
[https://zhuanlan.zhihu.com/p/104475016](https://zhuanlan.zhihu.com/p/104475016)  
[https://zhuanlan.zhihu.com/p/32085405](https://zhuanlan.zhihu.com/p/32085405) 
