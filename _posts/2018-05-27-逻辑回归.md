---
layout:     post
title:      "Logistic Regression"
subtitle:   "逻辑回归"
date:       2017-10-19 
author:     "xcTorres"
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - 机器学习
---
逻辑回归（Logistic Regression）是机器学习中的一种分类模型，由于算法的简单和高效，在实际中应用非常广泛。Logistic Regression（逻辑回归）。本文主要介绍了Logistic Regression（逻辑回归）模型的原理以及参数估计、公式推导方法。

# 模型构建
在介绍Logistic Regression之前我们先简单说一下线性回归，，线性回归的主要思想就是通过历史数据拟合出一条直线，用这条直线对新的数据进行预测。

我们知道，线性回归的公式如下：


$$
z={\theta_{0}}+{\theta_{1}x_{1}}+{\theta_{2}x_{2}+{\theta_{3}x_{3}}...+{\theta_{n}x_{n}}}=\theta^Tx
$$

而对于Logistic Regression来说，其思想也是基于线性回归（Logistic Regression属于广义线性回归模型）。其公式如下： 

$$
h_{\theta}(x)=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-\theta^Tx}}
$$

其中，

$$
y =  \frac{1}{1+e^{-x}}
$$

即为sigmoid函数，我们可以看到，Logistic Regression算法是将线性函数的结果映射到了sigmoid函数中。
sigmoid的函数图形如下： 


<div align="center"><img src="https://img-blog.csdn.net/20160730143626838"/></div>

</br>

我们可以看到，sigmoid的函数输出是介于（0，1）之间的，中间值是0.5，于是之前的公式 hθ(x)的含义就很好理解了，因为 hθ(x) 输出是介于（0，1）之间，也就表明了数据属于某一类别的概率，例如 ： 

hθ(x)<0.5 则说明当前数据属于A类； 
hθ(x)>0.5 则说明当前数据属于B类。

所以我们可以将sigmoid函数看成样本数据的概率密度函数。
有了上面的公式，我们接下来需要做的就是怎样去估计参数 θ 了。
首先我们来看， θ 函数的值有特殊的含义，它表示 hθ(x) 结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为： 

$$
P(y=1|x;\theta)=h_{\theta}(x)
$$

$$
P(y=0|x;\theta)=1-h_{\theta}(x)
$$


# 极大似然估计

根据上式，接下来我们可以使用概率论中极大似然估计的方法去求解损失函数，首先得到概率函数为： 

$$
P(y|x;\theta)={(h_{\theta}(x))^y} *{(1-h_{\theta}(x))^{1-y}}
$$


因为样本数据(m个)独立，所以它们的联合分布可以表示为各边际分布的乘积,取似然函数为： 

$$
L(\theta)=\prod_{i=1}^{m} P(y^{(i)}|x^{(i)};\theta)
$$

$$
L(\theta)=\prod_{i=1}^{m} {(h_{\theta}(x^{(i)}))^{y^{(i)}}} *{(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}}
$$

取对数似然函数： 

$$
l(\theta)=log(L(\theta))=\sum_{i=1}^{m} {y^{(i)}}log{(h_{\theta}(x^{(i)}))} + ({1-y^{(i)}})log{(1-h_{\theta}(x^{(i)}))}
$$

最大似然估计就是要求得使 l(θ) 取最大值时的 θ ，这里可以使用梯度上升法求解。我们稍微变换一下： 

$$
J(\theta)=-\frac{1}{m}l(\theta)
$$


因为乘了一个负的系数
$
-\frac{1}{m}
$
，然后就可以使用梯度下降算法进行参数求解了。


---
1.为什么要用sigmoid函数

除了说sigmoid函数能够满足将负无穷，与正无穷映射至[0,1]区间，且方便求导之外。还有个原因是，sigmoid函数的推导是伯努利分布的最大熵模型推导出来所得。

sigmoid函数如下表示。



最大熵模型：
[相关链接](https://homepages.inf.ed.ac.uk/lzhang10/maxent.html)

指数家族



sigmoid与softMax函数的联系及区别

---
2.我们知道损失函数的推导即是最大似然推导，那最大似然和最大后验概率法的区别是什么。

[MLE vs MAP](https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/)
即最大似然默认我们估计的参数都是符合均匀分布的，即出现概率相同。而最大后验概率不是，其有一些先验信息，即不同的参数发生的概率不同。但二者最终目的都是保证



---
3.梯度下降推导。










---
4.scikit-learn实践

```python
from sklearn import datasets
import numpy as np

iris = datasets.load_iris()
X = iris.data[:, [2, 3]]
y = iris.target

```


```python
##分开训练集和测试集
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

##数据归一化
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

```


```python
##线性回归
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=1000.0, random_state=0)
lr.fit(X_train_std, y_train)
```



```python
# Decision region drawing
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
%matplotlib inline
def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):
   # setup marker generator and color map
   markers = ('s', 'x', 'o', '^', 'v')
   colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
   cmap = ListedColormap(colors[:len(np.unique(y))])

   # plot the decision surface
   x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
   x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
   xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
   np.arange(x2_min, x2_max, resolution))
   Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
   Z = Z.reshape(xx1.shape)
   plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
   plt.xlim(xx1.min(), xx1.max())
   plt.ylim(xx2.min(), xx2.max())

   # 画出所有的样本
   X_test, y_test = X[test_idx, :], y[test_idx]
   for idx, cl in enumerate(np.unique(y)):
      plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
               alpha=0.8, c=cmap(idx),
               marker=markers[idx], label=cl)
   # highlight test samples
   if test_idx:
      X_test, y_test = X[test_idx, :], y[test_idx]
      plt.scatter(X_test[:, 0], X_test[:, 1], c='',
               alpha=1.0, linewidth=1, marker='o',
               s=55, label='test set')

```


```python
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

plot_decision_regions(X_combined_std,
                      y_combined, classifier=lr,
                      test_idx=range(105,150))

plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()
```


![png](Logistics%20Regression_files/Logistics%20Regression_4_0.png)


---