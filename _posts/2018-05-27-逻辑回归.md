---
layout:     post
title:      "Logistic Regression"
subtitle:   "逻辑回归"
date:       2018-5-27
author:     "xcTorres"
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - 机器学习
---
逻辑回归（Logistic Regression）是机器学习中的一种分类模型，由于算法的简单和高效，在实际中应用非常广泛。Logistic Regression（逻辑回归）。本文主要介绍了Logistic Regression（逻辑回归）模型的原理以及参数估计、公式推导方法。

## **模型构建**

在介绍Logistic Regression之前我们先简单说一下线性回归，，线性回归的主要思想就是通过历史数据拟合出一条直线，用这条直线对新的数据进行预测。

我们知道，线性回归的公式如下：


$$
z={\theta_{0}}+{\theta_{1}x_{1}}+{\theta_{2}x_{2}+{\theta_{3}x_{3}}...+{\theta_{n}x_{n}}}=\theta^Tx
$$

而对于Logistic Regression来说，其思想也是基于线性回归（Logistic Regression属于广义线性回归模型）。其公式如下：

$$
h_{\theta}(x)=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-\theta^Tx}}
$$

其中，

$$
y =  \frac{1}{1+e^{-x}}
$$

即为sigmoid函数，我们可以看到，Logistic Regression算法是将线性函数的结果映射到了sigmoid函数中。
sigmoid的函数图形如下：


<div align="center"><img src="https://img-blog.csdn.net/20160730143626838"/></div>


我们可以看到，sigmoid的函数输出是介于（0，1）之间的，中间值是0.5，于是之前的公式 hθ(x)的含义就很好理解了，因为 hθ(x) 输出是介于（0，1）之间，也就表明了数据属于某一类别的概率，例如 ：

hθ(x)<0.5 则说明当前数据属于A类；
hθ(x)>0.5 则说明当前数据属于B类。

所以我们可以将sigmoid函数看成样本数据的概率密度函数。
有了上面的公式，我们接下来需要做的就是怎样去估计参数 θ 了。
首先我们来看， θ 函数的值有特殊的含义，它表示 hθ(x) 结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为：

$$
P(y=1|x;\theta)=h_{\theta}(x)
$$

$$
P(y=0|x;\theta)=1-h_{\theta}(x)
$$

---
## **极大似然估计**

根据上式，接下来我们可以使用概率论中极大似然估计的方法去求解损失函数，首先得到概率函数为：
$$ P(y|x;\theta)={(h_{\theta}(x))^y}{(1-h_{\theta}(x))^{1-y}} $$



因为样本数据(m个)独立，所以它们的联合分布可以表示为各边际分布的乘积,取似然函数为：

$$
L(\theta)=\prod_{i=1}^{m} P(y^{(i)}|x^{(i)};\theta)
$$

$$
L(\theta)=\prod_{i=1}^{m} {(h_{\theta}(x^{(i)}))^{y^{(i)}}}{(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}}
$$

取对数似然函数：

$$
l(\theta)=log(L(\theta))=\sum_{i=1}^{m} {y^{(i)}}log{(h_{\theta}(x^{(i)}))} + ({1-y^{(i)}})log{(1-h_{\theta}(x^{(i)}))}
$$

最大似然估计就是要求得使 l(θ) 取最大值时的 θ ，这里可以使用梯度上升法求解。我们稍微变换一下：

$$
J(\theta)=-\frac{1}{m}l(\theta)
$$


因为乘了一个负的系数 $-\frac{1}{m}$，然后就可以使用梯度下降算法进行参数求解了。

---

## **梯度下降求导**

$$
l(\theta)=-\frac{1}{m}\sum_{i=1}^{m} {y^{(i)}}log{(h_{\theta}(x^{(i)}))} + ({1-y^{(i)}})log{(1-h_{\theta}(x^{(i)}))} \\

=-\frac{1}{m}\sum_{i=1}^{m}{y^{(i)}}log{\frac{h_{\theta}(x^{(i)}}{1-h_{\theta}(x^{(i)}}+log{(1-h_{\theta}(x^{(i)})})} \\
= -\frac{1}{m}\sum_{i=1}^{m}{y^{(i)}(w*x^{(i)})-log{(1+e^{w*x^{(i)}}})}
$$

$$
\frac{\partial{l(\theta)}}{w^{(j)}} = -\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x^{(ij)} \\
= \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(ij)}
$$

---
## **思考**

1.为什么要用sigmoid函数

1)sigmoid函数能够满足将负无穷，与正无穷映射至[0,1]区间

2)且方便求导。 $h_{\theta}^{'}(x)$  = $h_{\theta}(x) * (1-h_{\theta}(x))$

3)还有个原因,sigmoid函数的推导是伯努利分布的最大熵模型推导出来所得。

什么是最大熵模型：即在满足约束条件的模型集合中选出熵最大的模型
[Maximum Entrophy](https://www.cnblogs.com/pinard/p/6093948.html)

最大熵模型在分类方法里算是比较优的模型，但是由于它的约束函数的数目一般来说会随着样本量的增大而增大，导致样本量很大的时候，对偶函数优化求解的迭代过程非常慢，scikit-learn甚至都没有最大熵模型对应的类库。但是理解它仍然很有意义，尤其是它和很多分类方法都有千丝万缕的联系。　

　　　　惯例，我们总结下最大熵模型作为分类方法的优缺点：

　　　　最大熵模型的优点有：

　　　　a) 最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型,作为经典的分类模型时准确率较高。

　　　　b) 可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度

　　　　最大熵模型的缺点有：

　　　　c) 由于约束函数数量和样本数目有关系，导致迭代过程计算量巨大，实际应用比较难。

[最大熵模型推导sigmoid函数](https://link.zhihu.com/?target=http%3A//www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/)


---
2.sigmoid与softMax函数的联系及区别,交叉熵损失函数

---
3.我们知道损失函数的推导即是最大似然推导，那最大似然和最大后验概率法的区别是什么。

[MLE vs MAP](https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/)
即最大似然默认我们估计的参数都是符合均匀分布的，即出现概率相同。而最大后验概率不是，其有一些先验信息，即不同的参数发生的概率不同。但二者最终目的都是保证



---
4.梯度下降，随机梯度下降，小批量梯度下降的优缺点。


1） 批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新，也就是方程（1）中的m表示样本的所有个数。


优点：全局最优解；易于并行实现；

缺点：当样本数目很多时，训练过程会很慢。

2） 随机梯度下降法：它的具体思路是在更新每一参数时都使用一个样本来进行更新，也就是方程（1）中的m等于1。每一次跟新参数都用一个样本，更新很多次。如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次，这种跟新方式计算复杂度太高。

但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。

优点：训练速度快；

缺点：准确度下降，并不是全局最优；不易于并行实现。

从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。

3） 小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）：它的具体思路是在更新每一参数时都使用一部分样本来进行更新，也就是方程（1）中的m的值大于1小于所有样本的数量。为了克服上面两种方法的缺点，又同时兼顾两种方法的有点。

4）三种方法使用的情况：

如果样本量比较小，采用批量梯度下降算法。如果样本太大，或者在线算法，使用随机梯度下降算法。在实际的一般情况下，采用小批量梯度下降算法。







---
5.scikit-learn实践

```python
from sklearn import datasets
import numpy as np

iris = datasets.load_iris()
X = iris.data[:, [2, 3]]
y = iris.target

```


```python
##分开训练集和测试集
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

##数据归一化
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

```


```python
##线性回归
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=1000.0, random_state=0)
lr.fit(X_train_std, y_train)
```



```python
# Decision region drawing
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
%matplotlib inline
def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):
   # setup marker generator and color map
   markers = ('s', 'x', 'o', '^', 'v')
   colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
   cmap = ListedColormap(colors[:len(np.unique(y))])

   # plot the decision surface
   x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
   x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
   xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
   np.arange(x2_min, x2_max, resolution))
   Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
   Z = Z.reshape(xx1.shape)
   plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
   plt.xlim(xx1.min(), xx1.max())
   plt.ylim(xx2.min(), xx2.max())

   # 画出所有的样本
   X_test, y_test = X[test_idx, :], y[test_idx]
   for idx, cl in enumerate(np.unique(y)):
      plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
               alpha=0.8, c=cmap(idx),
               marker=markers[idx], label=cl)
   # highlight test samples
   if test_idx:
      X_test, y_test = X[test_idx, :], y[test_idx]
      plt.scatter(X_test[:, 0], X_test[:, 1], c='',
               alpha=1.0, linewidth=1, marker='o',
               s=55, label='test set')

```


```python
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

plot_decision_regions(X_combined_std,
                      y_combined, classifier=lr,
                      test_idx=range(105,150))

plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()
```


![](/img/in-post/logistics/result.png)


---
