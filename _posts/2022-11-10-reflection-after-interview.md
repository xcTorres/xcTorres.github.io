---
layout:     post
title:      "Reflection After Interview"
author:     "xcTorres"
header-img: "img/in-post/reflect.jpg"
catalog:    true
mathjax: true

tags:
    - Leetcode
---  

It has been 3 years working in Singapore since I graduated. During the period in Shopee, I learned a lot not only the coding skills but also the understanding of projects. Since Shopee has decided to shift the engineering center to China mainland, the projects that we Singapore team could do are becoming limited, which result in the thoughts to pursue other opportunities.  

There are not many opportunities avaliable in Singapore as expected, some local big company like Lazada and Grab freeze their head count, Amreican company like Google and facebook also don't have enough engineer head count. I got my permanent resident before I subimitted my CV, which helps me a lot to have more interview opportunities. Here are recent interview experience, unluckily I don't have any offer due to short of time to fully prepare.

# Interview  
#### Tiktok  
- **Machine learning engineer(recommendation)**  
Because of lack of recomendation-related experience, the interviewer in both 2 rounds doesn't have too much interests on my experience. So the coding problem appears to be very important，but I didn't prepare for a long time so coding performance sucks, failed the interview.  
  1st round: Leetcode 3(Longest Substring Without Repeating Characters)  
  2nd round: Leetcode 297(Serialize and Deserialize Binary Tree)

- **Machine learning engineer(platform governance)**  
Bert: how to pretrain; Q, K, V  
batch norm and layer norm  
How does GBDT do classfication?  
How to pretrain your own Bert?

#### Indeed  
The coding problem is easy, merge 2 sorted array. Here are some machine learning related problems.  

- Unfair coin(Bayesian probability)  
- Overfitting and underfitting  
- Variance, boosting and bagging  
- Dropout mechanisum


#### Apple
Coding: Top k frequent elements  
word2vec, Search System introduction, Bilstm-CRF


#### Huawei
leetcode: lognest valid parentheses, coin change  


#### Grab  
leetcode 1011

# Reflection  
To be honest, I am frustrated for a while, but I think I deserve it because I didn't fully prepare the coding and related machine learning knowledge on the CV. Right now I don't have chance and have to change my mind to look for new roles at the end of the year. It is not too bad because I have 4 months to prepare, this time I don't have any excuse. First I need to know which position I should apply for, I think machine learning engineer(search or NLP) should be more related, it is also okay if the job needs me to still work on map-related projects except assignment field.


# Preparation

#### Coding
[Leetcode](https://docs.google.com/spreadsheets/d/1l7Gvrubuscs0iwDPov053wGWV_uNg0yK_991JDeNIH0/edit#gid=2023823697)

#### Machine learning & Deep learning
- **Underfitting and Overfitting**  
  [link](https://www.cnblogs.com/zhhfan/p/10476761.html)
- **Bias and Variance**  
  [link](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)  
- Generative Model vs Discriminative Model
- Bagging vs Boost  
- Feature Selection  
- Bayesian Model  
- Logistic Regression  
- GBDT VS Xgboost VS LightGBM  
- **Metrics**: (MSE,MAE, Logloss, AUC curve)  
  [link](https://blog.csdn.net/Dby_freedom/article/details/89814644)  
- L1, L2 regularization  
- Hidden Markov Model  
- Conditional Random Field  
- **Activate function**  
  [link](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a)
- **Optimization algorithm**  
  [link](https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html)  
- **Batch Norm VS Layer Norm**  
  [link](https://bbs.huaweicloud.com/blogs/329066)


#### NLP  
- RNN  
- Gradient vanish & gradient explode
- **LSTM**  
  [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  
  [LSTM模型与前向反向传播算法](https://www.cnblogs.com/pinard/p/6519110.html)  
  [Solving the Vanishing Gradient Problem with LSTM](https://www.codingninjas.com/codestudio/library/solving-the-vanishing-gradient-problem-with-lstm)
- Word2vec  
- **Bert**  
  1) [The number of Bert parameters](https://zhuanlan.zhihu.com/p/144582114)  
  2) [Why does self-attention divide root of dk](https://blog.csdn.net/suibianshen2012/article/details/122141294)  
  3) [The comparison of all attentions]
  

- **Bi-lstm + CRF**  
  [CRF Layer on the Top of BiLSTM](https://createmomo.github.io/)  
  [Pytorch Code](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html)

#### Recommendation  
- FM  
- FFM
- DeepFM  
- DSSM


#### Rank


#### Graph