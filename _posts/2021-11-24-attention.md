---
layout:     post
title:      "Attention机制与Transformer模型"
date:       2021-11-24
author:     "xcTorres"
header-img: "img/in-post/machine-learning.jpeg"
catalog:    true
mathjax: true
tags:
    - NLP
    - Machine Learning
---  

如果想要了解Attention机制，最重要的当然是先读[论文](https://arxiv.org/abs/1706.03762)了，不过原始论文非常简练，很难一次读懂, 感谢李沐老师逐段讲解论文，非常清晰，强烈推荐[Transformer论文逐段精读](https://www.bilibili.com/video/BV1pu411o7BE?spm_id_from=333.999.0.0)，其次就是可以看一下源码实现，如下是Pytorch的实现源码。[https://github.com/jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)    

<img src="/img/in-post/attention/attention.png" width="500"/>



## Reference  
[Attention is all you need: Discovering the Transformer paper](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634)




