---
layout:     post
title:      "机器学习概念总结"
subtitle:   "逻辑回归 支持向量机"
date:       2017-11-11
author:     "xcTorres"
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - 机器学习
---
#
1. 熵：是对平均不确定性的度量，衡量差异性。

```math
H(X) = -\sum_{x\in X}P(x).logP(x)
```
#
2.信息增益(互信息)：一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。衡量相似性。
#
3.决策树：自顶向下的递归方法其基本思想是以信息熵为度量，构建一颗熵下降最快的树，到叶子节点时熵为零。

常用算法有ID3（信息增益），C4.5(信息增益率)，CART(Gini系数)

联系以及区别：


决策树对训练数据有很好的分类能力。泛化能力较弱，容易产生过拟合现象。（剪枝，随机森林）


信息增益率：

```math
g_r(D,A) = g(D,A)/H(A)
```
基尼指数：
```math
Gini(p) = \sum_{k=1}^Kp_k(1-pk)=1-\sum_{k=1}^Kp_k^2=1-\sum_{k=1}^K(\frac{\left|C_k\right|}{D})^2
```
#
6.Logistic Regression与SVM的联系以及区别：

两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。

这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。

两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。还有很重要的一点，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。

两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多的话，无法剔除，首先LR，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。
#

4. AdaBoost算法：

5. GBDT算法

1.svm推导

2.bp算法，梯度弥散问题

3.EM算法原理

4.SVM推导，为什么要用拉格朗日乘法，什么是对偶问题

5.LR与线性回归的区别


6.sigmoid函数的应用有哪些，为什么

7.softmax的损失函数是什么

8.决策树算法有哪些。

9.随机森林与GBDT算法的区别

10.最大似然估计与最大后验的区别

11.异常值的影响，如何剔除

12.均匀分布如何生成正态分布

